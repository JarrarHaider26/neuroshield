#!/usr/bin/env python3
"""
NeuroShield Malware Scanner - Corrected Feature Extraction
Extracts features from PE files EXACTLY as NeuroShield does during training.
"""

import os
import sys
import json
import pickle
import logging
import hashlib
import math
from collections import Counter
from typing import Dict, Any, Optional, List
import numpy as np
import pandas as pd

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ByteHistogram:
    """
    NeuroShield-style byte histogram extractor.
    Returns RAW byte counts (not normalized) - StandardScaler handles normalization.
    """
    def __init__(self):
        self.dim = 256
    
    def extract(self, bytez: bytes) -> List[int]:
        """Extract raw byte histogram counts."""
        counts = np.bincount(np.frombuffer(bytez, dtype=np.uint8), minlength=256)
        return counts.tolist()  # Return RAW counts, not normalized!


class ByteEntropyHistogram:
    """
    NeuroShield-style byte entropy histogram.
    Returns RAW counts of entropy buckets (not normalized).
    OPTIMIZED: Sample every 4KB instead of every 1KB for faster processing.
    """
    def __init__(self, step: int = 4096, window: int = 2048):  # Increased step size
        self.step = step
        self.window = window
        self.dim = 256
    
    def extract(self, bytez: bytes) -> List[int]:
        """Extract raw byte entropy histogram counts."""
        if len(bytez) < self.window:
            return [0] * self.dim
        
        # Calculate entropy for each window
        output = np.zeros(self.dim, dtype=np.int32)
        
        # Limit to first 10MB for very large files
        max_bytes = min(len(bytez), 10 * 1024 * 1024)
        
        for start in range(0, max_bytes - self.window + 1, self.step):
            window_bytes = bytez[start:start + self.window]
            # Calculate entropy
            counts = np.bincount(np.frombuffer(window_bytes, dtype=np.uint8), minlength=256)
            counts = counts[counts > 0]  # Remove zeros
            probs = counts / counts.sum()
            entropy = -np.sum(probs * np.log2(probs))
            
            # Bin entropy (0-8) into 256 buckets
            bucket = int(entropy * 32)  # 8 * 32 = 256
            bucket = min(255, max(0, bucket))
            output[bucket] += 1
        
        # Return RAW counts, not normalized - StandardScaler handles this!
        return output.tolist()


class StringExtractor:
    """
    NeuroShield-style string feature extractor.
    """
    def __init__(self):
        self.dim = 104  # 8 scalar + 96 printable dist
    
    def extract(self, bytez: bytes) -> Dict[str, Any]:
        """Extract string-based features."""
        # Extract printable strings (ASCII 32-126)
        strings = []
        current = []
        
        for b in bytez:
            if 32 <= b <= 126:
                current.append(chr(b))
            else:
                if len(current) >= 5:  # NeuroShield uses min length 5
                    strings.append(''.join(current))
                current = []
        if len(current) >= 5:
            strings.append(''.join(current))
        
        # Calculate features
        features = {}
        
        # Number of strings
        features['strings_numstrings'] = len(strings)
        
        # Average length
        if strings:
            features['strings_avlength'] = sum(len(s) for s in strings) / len(strings)
        else:
            features['strings_avlength'] = 0.0
        
        # Printable characters count
        features['strings_printables'] = sum(1 for b in bytez if 32 <= b <= 126)
        
        # String entropy
        all_strings = ''.join(strings)
        if all_strings:
            counts = Counter(all_strings)
            total = len(all_strings)
            entropy = -sum((c/total) * math.log2(c/total) for c in counts.values())
            features['strings_entropy'] = entropy
        else:
            features['strings_entropy'] = 0.0
        
        # Path count (backslash and forward slash)
        all_text = bytez.decode('latin-1', errors='ignore')
        features['strings_paths'] = all_text.count('\\') + all_text.count('/')
        
        # URL count
        features['strings_urls'] = all_text.lower().count('http://') + all_text.lower().count('https://')
        
        # Registry count
        features['strings_registry'] = all_text.lower().count('hkey_')
        
        # MZ header count
        features['strings_MZ'] = all_text.count('MZ')
        
        # Printable character distribution (96 chars: 32-127)
        # Return RAW counts - StandardScaler handles normalization
        printable_dist = [0] * 96
        for b in bytez:
            if 32 <= b <= 127:
                printable_dist[b - 32] += 1
        
        for i in range(96):
            features[f'strings_printabledist_{i}'] = printable_dist[i]
        
        return features


class GeneralFileInfo:
    """
    NeuroShield-style general PE info extractor.
    """
    def extract(self, pe) -> Dict[str, Any]:
        """Extract general PE features."""
        features = {}
        
        # File size
        features['general_size'] = pe.virtual_size if hasattr(pe, 'virtual_size') else 0
        features['general_vsize'] = pe.virtual_size if hasattr(pe, 'virtual_size') else 0
        
        # Debug info
        features['general_has_debug'] = 1 if pe.has_debug else 0
        
        # Exports
        if hasattr(pe, 'exported_functions'):
            features['general_exports'] = len(list(pe.exported_functions))
        else:
            features['general_exports'] = 0
        
        # Imports
        if pe.imports:
            features['general_imports'] = sum(len(lib.entries) for lib in pe.imports)
        else:
            features['general_imports'] = 0
        
        # Other flags
        features['general_has_relocations'] = 1 if pe.has_relocations else 0
        features['general_has_resources'] = 1 if pe.has_resources else 0
        features['general_has_signature'] = 1 if (hasattr(pe, 'signatures') and len(list(pe.signatures)) > 0) else 0
        features['general_has_tls'] = 1 if pe.has_tls else 0
        features['general_symbols'] = len(list(pe.symbols)) if hasattr(pe, 'symbols') and pe.symbols else 0
        
        return features


class HeaderInfo:
    """
    NeuroShield-style PE header extractor.
    """
    def extract(self, pe) -> Dict[str, Any]:
        """Extract PE header features."""
        features = {}
        
        # COFF timestamp
        if hasattr(pe, 'header') and hasattr(pe.header, 'time_date_stamps'):
            features['header_coff_timestamp'] = pe.header.time_date_stamps
        else:
            features['header_coff_timestamp'] = 0
        
        # Optional header fields
        opt = pe.optional_header if hasattr(pe, 'optional_header') else None
        
        header_fields = [
            'major_image_version', 'minor_image_version',
            'major_linker_version', 'minor_linker_version',
            'major_operating_system_version', 'minor_operating_system_version',
            'major_subsystem_version', 'minor_subsystem_version',
            'sizeof_code', 'sizeof_headers', 'sizeof_heap_commit'
        ]
        
        for field in header_fields:
            if opt and hasattr(opt, field):
                features[f'header_optional_{field}'] = getattr(opt, field)
            else:
                features[f'header_optional_{field}'] = 0
        
        return features


class SectionInfo:
    """
    NeuroShield-style PE section extractor.
    """
    def extract(self, pe) -> Dict[str, Any]:
        """Extract section features (8 sections max)."""
        features = {}
        sections = list(pe.sections) if pe.sections else []
        
        for i in range(8):
            if i < len(sections):
                sec = sections[i]
                features[f'section_{i}_size'] = sec.size if hasattr(sec, 'size') else 0
                features[f'section_{i}_entropy'] = sec.entropy if hasattr(sec, 'entropy') else 0.0
                features[f'section_{i}_vsize'] = sec.virtual_size if hasattr(sec, 'virtual_size') else 0
            else:
                features[f'section_{i}_size'] = 0
                features[f'section_{i}_entropy'] = 0.0
                features[f'section_{i}_vsize'] = 0
        
        return features


class DataDirectories:
    """
    NeuroShield-style data directory extractor.
    """
    def extract(self, pe) -> Dict[str, Any]:
        """Extract data directory features (15 directories)."""
        features = {}
        
        data_dirs = list(pe.data_directories) if hasattr(pe, 'data_directories') and pe.data_directories else []
        
        for i in range(15):
            if i < len(data_dirs):
                dd = data_dirs[i]
                features[f'datadirectory_{i}_size'] = dd.size if hasattr(dd, 'size') else 0
                features[f'datadirectory_{i}_virtual_address'] = dd.rva if hasattr(dd, 'rva') else 0
            else:
                features[f'datadirectory_{i}_size'] = 0
                features[f'datadirectory_{i}_virtual_address'] = 0
        
        return features


class PEFeatureExtractor:
    """
    Complete NeuroShield-compatible PE feature extractor.
    Extracts exactly 692 features matching the training format.
    """
    
    def __init__(self):
        """Initialize all feature extractors."""
        try:
            import lief
            self.lief = lief
            # Disable LIEF logging
            lief.logging.disable()
        except ImportError:
            raise ImportError("LIEF library required. Install with: pip install lief")
        
        self.histogram = ByteHistogram()
        self.byteentropy = ByteEntropyHistogram()
        self.strings = StringExtractor()
        self.general = GeneralFileInfo()
        self.header = HeaderInfo()
        self.sections = SectionInfo()
        self.datadirs = DataDirectories()
        
        logger.info("PE Feature Extractor initialized")
    
    def extract(self, file_path: str) -> Optional[Dict[str, Any]]:
        """
        Extract all features from a PE file.
        
        Args:
            file_path: Path to the PE file
            
        Returns:
            Dictionary with 692 features
        """
        try:
            # Read raw bytes
            with open(file_path, 'rb') as f:
                bytez = f.read()
            
            # Parse PE
            pe = self.lief.parse(file_path)
            if pe is None:
                logger.error(f"LIEF failed to parse: {file_path}")
                return None
            
            features = {}
            
            # Extract histogram (256 features)
            hist = self.histogram.extract(bytez)
            for i, val in enumerate(hist):
                features[f'histogram_{i}'] = val
            
            # Extract byte entropy (256 features)
            entropy = self.byteentropy.extract(bytez)
            for i, val in enumerate(entropy):
                features[f'byteentropy_{i}'] = val
            
            # Extract string features (104 features)
            features.update(self.strings.extract(bytez))
            
            # Extract general info (10 features)
            features.update(self.general.extract(pe))
            
            # Extract header info (12 features)
            features.update(self.header.extract(pe))
            
            # Extract section info (24 features)
            features.update(self.sections.extract(pe))
            
            # Extract data directories (30 features)
            features.update(self.datadirs.extract(pe))
            
            logger.debug(f"Extracted {len(features)} features")
            return features
            
        except Exception as e:
            logger.error(f"Feature extraction failed: {e}")
            import traceback
            traceback.print_exc()
            return None


class MalwareScanner:
    """
    Complete malware scanning solution using trained NeuroShield model.
    """
    
    def __init__(self, model_path: str = "xgboost_model.pkl", scaler_path: str = "scaler.pkl"):
        """
        Initialize the scanner.
        
        Args:
            model_path: Path to trained XGBoost model
            scaler_path: Path to StandardScaler file
        """
        self.model = None
        self.scaler = None
        self.feature_names = None
        self.extractor = None
        
        self._load_model(model_path)
        self._load_scaler(scaler_path)
        self.extractor = PEFeatureExtractor()
    
    def _load_model(self, model_path: str):
        """Load the trained model using joblib."""
        import joblib
        import warnings
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            self.model = joblib.load(model_path)
        logger.info(f"Model loaded: {type(self.model).__name__}")
        logger.info(f"Expected features: {getattr(self.model, 'n_features_in_', 'unknown')}")
    
    def _load_scaler(self, scaler_path: str):
        """Load the StandardScaler using joblib."""
        import joblib
        import warnings
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            self.scaler = joblib.load(scaler_path)
        
        # Get feature names from scaler
        if hasattr(self.scaler, 'feature_names_in_'):
            self.feature_names = list(self.scaler.feature_names_in_)
        else:
            # Generate default feature names matching EMBER format
            self.feature_names = self._generate_feature_names()
        
        logger.info(f"Scaler loaded: {type(self.scaler).__name__}")
        logger.info(f"Features: {len(self.feature_names)}")
    
    def _generate_feature_names(self) -> List[str]:
        """Generate feature names matching NeuroShield training format."""
        names = []
        # Histogram (256)
        names.extend([f'histogram_{i}' for i in range(256)])
        # Byte entropy (256)
        names.extend([f'byteentropy_{i}' for i in range(256)])
        # Strings (8 + 96 = 104)
        names.extend(['strings_numstrings', 'strings_avlength', 'strings_printables',
                     'strings_entropy', 'strings_paths', 'strings_urls',
                     'strings_registry', 'strings_MZ'])
        names.extend([f'strings_printabledist_{i}' for i in range(96)])
        # General (10)
        names.extend(['general_size', 'general_vsize', 'general_has_debug',
                     'general_exports', 'general_imports', 'general_has_relocations',
                     'general_has_resources', 'general_has_signature',
                     'general_has_tls', 'general_symbols'])
        # Header (12)
        names.append('header_coff_timestamp')
        names.extend([f'header_optional_{k}' for k in [
            'major_image_version', 'minor_image_version', 'major_linker_version',
            'minor_linker_version', 'major_operating_system_version',
            'minor_operating_system_version', 'major_subsystem_version',
            'minor_subsystem_version', 'sizeof_code', 'sizeof_headers',
            'sizeof_heap_commit'
        ]])
        # Sections (24)
        for i in range(8):
            names.extend([f'section_{i}_size', f'section_{i}_entropy', f'section_{i}_vsize'])
        # Data directories (30)
        for i in range(15):
            names.extend([f'datadirectory_{i}_size', f'datadirectory_{i}_virtual_address'])
        return names
    
    def _to_vector(self, features: Dict[str, Any]) -> np.ndarray:
        """Convert feature dict to ordered numpy array and apply scaling."""
        vector = []
        for name in self.feature_names:
            val = features.get(name, 0)
            if val is None:
                val = 0
            vector.append(float(val))
        
        # Convert to DataFrame with feature names (required by scaler)
        df = pd.DataFrame([vector], columns=self.feature_names)
        
        # Apply StandardScaler transformation (CRITICAL for correct predictions!)
        vector_scaled = self.scaler.transform(df)
        
        return vector_scaled
    
    def scan(self, file_path: str) -> Dict[str, Any]:
        """
        Scan a PE file for malware.
        
        Args:
            file_path: Path to the PE file
            
        Returns:
            Scan results dictionary
        """
        result = {
            "file_path": file_path,
            "file_name": os.path.basename(file_path),
            "file_hash": None,
            "file_size": None,
            "verdict": "Error",
            "confidence": 0.0,
            "malware_probability": 0.0,
            "benign_probability": 0.0,
            "threat_severity": "Unknown",
            "error": None,
            "timestamp": pd.Timestamp.now().isoformat()
        }
        
        try:
            # Check file exists
            if not os.path.exists(file_path):
                result["error"] = "File not found"
                return result
            
            # Get file info
            result["file_size"] = os.path.getsize(file_path)
            with open(file_path, 'rb') as f:
                result["file_hash"] = hashlib.sha256(f.read()).hexdigest()
            
            # Extract features
            logger.info(f"Extracting features from {file_path}...")
            features = self.extractor.extract(file_path)
            
            if features is None:
                result["error"] = "Failed to extract features - invalid PE file"
                return result
            
            # Convert to vector and apply scaling
            vector_scaled = self._to_vector(features)
            
            # Verify feature count
            expected = self.model.n_features_in_
            if vector_scaled.shape[1] != expected:
                result["error"] = f"Feature mismatch: got {vector_scaled.shape[1]}, expected {expected}"
                return result
            
            # Make prediction (vector is already scaled and reshaped)
            try:
                probabilities = self.model.predict_proba(vector_scaled)[0]
            except AttributeError as e:
                # Fallback for XGBoost compatibility issues
                logger.warning(f"predict_proba failed: {e}, using booster")
                import xgboost as xgb
                dmatrix = xgb.DMatrix(vector_scaled, feature_names=self.feature_names)
                raw_pred = self.model.get_booster().predict(dmatrix)[0]
                probabilities = [1 - raw_pred, raw_pred]
            
            # Custom threshold: 7% (0.07) - if malware probability >= 7%, classify as malicious
            MALWARE_THRESHOLD = 0.07
            malware_prob = float(probabilities[1])
            is_malware = malware_prob >= MALWARE_THRESHOLD
            
            # Confidence is the probability of the predicted class
            confidence = malware_prob if is_malware else float(probabilities[0])
            
            result["verdict"] = "Malicious" if is_malware else "Benign"
            result["confidence"] = round(confidence, 4)
            result["malware_probability"] = round(malware_prob, 4)
            result["benign_probability"] = round(float(probabilities[0]), 4)
            
            # Threat severity based on malware probability
            if is_malware:
                if malware_prob >= 0.50:
                    result["threat_severity"] = "Critical"
                elif malware_prob >= 0.35:
                    result["threat_severity"] = "High"
                elif malware_prob >= 0.20:
                    result["threat_severity"] = "Medium"
                else:
                    result["threat_severity"] = "Low"
            else:
                result["threat_severity"] = "None"
            
            logger.info(f"Scan complete: {result['verdict']} ({confidence:.2%})")
            
        except Exception as e:
            logger.error(f"Scan failed: {e}")
            import traceback
            traceback.print_exc()
            result["error"] = str(e)
        
        return result
    
    def get_info(self) -> Dict[str, Any]:
        """Get scanner information."""
        return {
            "model_type": type(self.model).__name__,
            "feature_count": len(self.feature_names),
            "expected_features": getattr(self.model, 'n_features_in_', 'unknown'),
            "classes": getattr(self.model, 'classes_', [0, 1]).tolist()
        }


def main():
    """CLI interface."""
    import argparse
    
    parser = argparse.ArgumentParser(description="NeuroShield Malware Scanner")
    parser.add_argument("file", nargs="?", help="PE file to scan")
    parser.add_argument("--model", default="xgboost_model.pkl", help="Model file")
    parser.add_argument("--scaler", default="scaler.pkl", help="Feature names file")
    parser.add_argument("--info", action="store_true", help="Show scanner info")
    parser.add_argument("--json", action="store_true", help="Output as JSON")
    parser.add_argument("--debug", action="store_true", help="Enable debug logging")
    
    args = parser.parse_args()
    
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
    
    try:
        scanner = MalwareScanner(args.model, args.scaler)
        
        if args.info:
            info = scanner.get_info()
            print(json.dumps(info, indent=2) if args.json else info)
            return
        
        if not args.file:
            print("Usage: python malware_scanner.py <file.exe>")
            print("       python malware_scanner.py --info")
            return
        
        result = scanner.scan(args.file)
        
        if args.json:
            print(json.dumps(result, indent=2))
        else:
            print("\n" + "=" * 50)
            print("NEUROSHIELD MALWARE SCAN RESULTS")
            print("=" * 50)
            print(f"File:     {result['file_name']}")
            print(f"Size:     {result['file_size']:,} bytes")
            print(f"SHA256:   {result['file_hash'][:16]}...")
            print("-" * 50)
            print(f"VERDICT:  {result['verdict']}")
            print(f"Confidence: {result['confidence']:.2%}")
            print(f"Malware Probability: {result['malware_probability']:.2%}")
            print(f"Threat Severity: {result['threat_severity']}")
            if result['error']:
                print(f"Error: {result['error']}")
            print("=" * 50)
            
    except Exception as e:
        logger.error(f"Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()